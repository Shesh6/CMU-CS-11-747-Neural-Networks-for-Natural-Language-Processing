{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-Linear Language Model from Scratch\n",
    "Exercise 4-6 from Neural Machine Translation and Sequence-to-sequence Models: A Tutorial by \n",
    "Graham Neubig (https://arxiv.org/abs/1703.01619)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# length of past-window\n",
    "n = 3\n",
    "\n",
    "# load data\n",
    "X_train = open(\"en-de/train.en-de.low.filt.en\", \"r\", encoding=\"UTF-8\").readlines()\n",
    "X_test = open(\"en-de/test.en-de.low.en\", \"r\", encoding=\"UTF-8\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode vocab as numerical index\n",
    "padded_train = []\n",
    "# automatically register new words to new consequitive ids\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "# define special characters\n",
    "S = w2i[\"<s>\"]\n",
    "END = w2i[\"</s>\"]\n",
    "UNK = w2i[\"<UNK>\"]\n",
    "\n",
    "# pad, split, and encode training data (taking subset to avoid dimensionality explosion)\n",
    "for sentence in X_train[:1000]:\n",
    "    padded_train.append([S]*n)\n",
    "    for word in sentence.strip().split():\n",
    "        padded_train[-1].append(w2i[word])\n",
    "    padded_train[-1].append(END)\n",
    "\n",
    "VOCAB = list(w2i.keys())\n",
    "i2w = {v: k for k,v in w2i.items()}\n",
    "\n",
    "# vocab length\n",
    "V = len(VOCAB)\n",
    "# size of embedding vector, depends on feature function defined below\n",
    "N = n*V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_test = []\n",
    "\n",
    "# encode uknown words as UNK token\n",
    "w2i.default_factory = lambda: UNK\n",
    "\n",
    "# pad, split, and encode test data\n",
    "for sentence in X_test:\n",
    "        padded_test.append([S]*n + [w2i[word] for word in sentence.strip().split()] + [END])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax function\n",
    "def softmax(scores):\n",
    "    exp = np.exp(scores)\n",
    "    return exp/np.sum(exp)\n",
    "\n",
    "# log-likelihood loss\n",
    "def LL_loss(prob):\n",
    "    loss = -np.log(prob)\n",
    "    return loss\n",
    "\n",
    "# convert indices into one-hot encoding matrix, given vocab size\n",
    "def onehot(indices, size):\n",
    "    indices = np.array(indices)\n",
    "    # creates n x V zeros matrix\n",
    "    oh_array = np.zeros((size,indices.size))\n",
    "    # set entries of indices to 1\n",
    "    oh_array[indices,np.arange(indices.size)] = 1\n",
    "    # return one-hot array\n",
    "    return oh_array\n",
    "\n",
    "# feature function to turn input into feature vector\n",
    "# concatenated OHE for words in the window, R^N = R^{n*V X 1}\n",
    "def feat_func(window):\n",
    "    oh_mat = onehot(window, V)\n",
    "    concatenated = oh_mat.T[::-1].reshape(-1,1)\n",
    "    return concatenated\n",
    "\n",
    "# forward pass, compute scores from input given parameters\n",
    "def forward(x, W, b):\n",
    "    # affine transformation\n",
    "    # scores = W @ x + b.reshape(-1,1)\n",
    "\n",
    "    # lookup, more computationally efficient for sparse matrices (such as one-hot)\n",
    "    # W[:,j]            \\in R^{V X 1}\n",
    "    # x[j]              \\in R^{1}\n",
    "    # b                 \\in R^{V X 1}\n",
    "    # s                 \\in R^{V X 1} = \\sum W[:,j] * x[j] + b\n",
    "    score = np.sum([W[:,j] * x[j] + b for j in range(N) if x[j] != 0],0)\n",
    "    return score\n",
    "\n",
    "# optimization step\n",
    "def step(sentence, W, b, eta = 0.1):\n",
    "    # convert input to feature vector\n",
    "    x = feat_func(sentence[:-1])\n",
    "    # compute probabilities from feature vector and parameters\n",
    "    probs = softmax(forward(x, W, b))\n",
    "    # compute the gradients for biases and weights and take optimization step\n",
    "    dldb = probs.T - onehot(sentence[-1], V).flatten()\n",
    "    b_step = np.multiply(dldb,eta)\n",
    "    b -= b_step\n",
    "    # lookup active weight rows to update\n",
    "    for j in range(N):\n",
    "        if np.sum(x[j]) != 0:\n",
    "            W[:,j] -= b_step*x[j]\n",
    "    return W, b\n",
    "\n",
    "def train(X):\n",
    "    # initialize parameters\n",
    "    W = np.random.rand(V,N)\n",
    "    b = np.random.rand(V)\n",
    "    # for each word in the training data (except padding)\n",
    "    for sentence in tqdm(X):\n",
    "        for i in range(n+1, len(sentence)+1):\n",
    "            # compute step and update parameters\n",
    "            W, b = step(sentence[i-n-1:i], W, b)\n",
    "    # in case of unknown word, assume uniform distribution over 1e6 words\n",
    "    for i in range(UNK,N+UNK,V):\n",
    "        W[:,i] = np.array([1e-6]*V)\n",
    "    # return parameters\n",
    "    return (W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [13:58<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train model   \n",
    "W, b = train(padded_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:23<00:00,  4.92it/s]\n",
      "100%|██████████| 1565/1565 [05:30<00:00,  4.74it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# score model on train and test sets\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# for each word in each sentence\n",
    "for sentence in tqdm(padded_train):\n",
    "    sentence_loss = 0\n",
    "    # compute probability of word according to model\n",
    "    for i in range(n+1,len(sentence)):\n",
    "            # convert window to feature vector\n",
    "            x = feat_func(sentence[i-n-1:i-1])\n",
    "            # compute the loss according to the probability given to the label by the model\n",
    "            sentence_loss += LL_loss(softmax(forward(x, W, b))[sentence[i]])\n",
    "    # normalize by length of sentence\n",
    "    train_losses.append(sentence_loss/len(sentence))\n",
    "\n",
    "# as above\n",
    "for sentence in tqdm(padded_test):\n",
    "    sentence_loss = 0\n",
    "    for i in range(n+1,len(sentence)):\n",
    "            x = feat_func(sentence[i-n-1:i-1])\n",
    "            sentence_loss += LL_loss(softmax(forward(x, W, b))[sentence[i]])\n",
    "    test_losses.append(sentence_loss/len(sentence))\n",
    "\n",
    "# plot losses as histograms\n",
    "_ = plt.hist(train_losses,100,density=True,alpha=0.75,label=\"Train\")\n",
    "_ = plt.hist(test_losses,100,density=True,alpha=0.75,label=\"Test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prediction functions\n",
    "\n",
    "def greedy_search_step(past, W, b):\n",
    "    # compute score vector and choose highest score\n",
    "    x = feat_func(past[-n:])\n",
    "    scores = forward(x, W, b)\n",
    "    # prevent choosing padding token\n",
    "    scores[0] = 0\n",
    "    return np.argmax(scores)\n",
    "\n",
    "def random_search_step(past, W, b):\n",
    "    # compute score vector and sample from softmaxed distribution\n",
    "    x = feat_func(past[-n:])\n",
    "    scores = forward(x, W, b)\n",
    "    # prevent choosing padding token\n",
    "    scores[0] = 0\n",
    "    probs = softmax(scores)\n",
    "    return np.random.choice(range(V), p = probs)\n",
    "\n",
    "# predict sentence given seed and length\n",
    "def search(W, b, seed, length, strat=\"greedy\"):\n",
    "    sentence = [w2i[word] for word in seed.split()]\n",
    "    for _ in range(length):\n",
    "        # halt if end sentence token is reached\n",
    "        if sentence[-1] == END:\n",
    "            return \" \".join([i2w[i] for i in sentence])\n",
    "        # predict next word\n",
    "        if strat == \"greedy\":\n",
    "            prediction = greedy_search_step(sentence, W, b)\n",
    "        if strat == \"random\":\n",
    "            prediction = random_search_step(sentence, W, b)\n",
    "        # append to sentence\n",
    "        sentence.append(prediction)\n",
    "    return \" \".join([i2w[i] for i in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the little man had , and the was of the in the , and\n",
      "the little man had adam versions . </s>\n"
     ]
    }
   ],
   "source": [
    "# test generation\n",
    "\n",
    "seed = \"the little man had\"\n",
    "length = 10\n",
    "print(search(W, b, seed, length, \"greedy\"))\n",
    "print(search(W, b, seed, length, \"random\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the little woman had . </s>\n",
      "the little woman had which room end an bloodstream in eating , . get\n"
     ]
    }
   ],
   "source": [
    "seed = \"the little woman had\"\n",
    "length = 10\n",
    "print(search(W, b, seed, length, \"greedy\"))\n",
    "print(search(W, b, seed, length, \"random\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fin."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9026a92a5d7f7d8aa1c4ab30d4b010c040491f6b59d39fcb84365f121721c1f0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('coca1227': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
