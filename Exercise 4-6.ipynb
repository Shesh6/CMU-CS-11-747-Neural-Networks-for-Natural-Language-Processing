{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-Linear Language Model from Scratch\n",
    "Exercise 4-6 from Neural Machine Translation and Sequence-to-sequence Models: A Tutorial by \n",
    "Graham Neubig (https://arxiv.org/abs/1703.01619)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# maximum n-gram length\n",
    "n = 2\n",
    "\n",
    "# load data\n",
    "X_train = open(\"en-de/train.en-de.low.filt.en\", \"r\", encoding=\"UTF-8\").readlines()\n",
    "X_test = open(\"en-de/test.en-de.low.en\", \"r\", encoding=\"UTF-8\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode vocab as numerical index\n",
    "padded_train = []\n",
    "# automatically register new words to new consequitive ids\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "# define special characters\n",
    "S = w2i[\"<s>\"]\n",
    "END = w2i[\"</s>\"]\n",
    "UNK = w2i[\"<UNK>\"]\n",
    "\n",
    "# pad, split, and encode training data (taking subset to avoid dimensionality explosion)\n",
    "for sentence in X_train[:1000]:\n",
    "    padded_train.append([S]*n)\n",
    "    for word in sentence.strip().split():\n",
    "        padded_train[-1].append(w2i[word])\n",
    "    padded_train[-1].append(END)\n",
    "\n",
    "VOCAB = list(w2i.keys())\n",
    "i2w = {v: k for k,v in w2i.items()}\n",
    "\n",
    "# vocab length\n",
    "V = len(VOCAB)\n",
    "# size of embedding vector, depends on feature function defined below\n",
    "N = n*V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_test = []\n",
    "\n",
    "# encode uknown words as UNK token\n",
    "w2i.default_factory = lambda: UNK\n",
    "\n",
    "# pad, split, and encode test data\n",
    "for sentence in X_test:\n",
    "        padded_test.append([S]*n + [w2i[word] for word in sentence.strip().split()] + [END])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax function\n",
    "def softmax(scores):\n",
    "    exp = np.exp(scores)\n",
    "    return exp/np.sum(exp)\n",
    "\n",
    "# log-likelihood loss\n",
    "def LL_loss(prob):\n",
    "    loss = -np.log(prob)\n",
    "    return loss\n",
    "\n",
    "# convert indices into one-hot encoding matrix, given vocab size\n",
    "def onehot(indices, size):\n",
    "    indices = np.array(indices)\n",
    "    # creates n x V zeros matrix\n",
    "    oh_array = np.zeros((size,indices.size))\n",
    "    # set entries of indices to 1\n",
    "    oh_array[indices,np.arange(indices.size)] = 1\n",
    "    # return one-hot array\n",
    "    return oh_array\n",
    "\n",
    "# feature function to turn input into feature vector\n",
    "# concatenated OHE for words in the window, N \\in R^{n*V X 1}\n",
    "def feat_func(window):\n",
    "    oh_mat = onehot(window, V)\n",
    "    concatenated = oh_mat.T[::-1].reshape(-1,1)\n",
    "    return concatenated\n",
    "\n",
    "# forward pass, compute scores from input given parameters\n",
    "def forward(x, W, b):\n",
    "    # affine transformation\n",
    "    # scores = W @ x + b.reshape(-1,1)\n",
    "\n",
    "    # lookup, more computationally efficient for sparse matrices (such as one-hot)\n",
    "    # W[:,j]            \\in R^{V X 1}\n",
    "    # x[j]              \\in R^{1}\n",
    "    # b                 \\in R^{V X 1}\n",
    "    # s                 \\in R^{V X 1} = \\sum W[:,j] * x[j] + b\n",
    "    score = np.sum([W[:,j] * x[j] + b for j in range(N) if x[j] != 0],0)\n",
    "    return score\n",
    "\n",
    "# optimization step\n",
    "def step(sentence, W, b, eta = 0.1):\n",
    "    # convert input to feature vector\n",
    "    x = feat_func(sentence[:-1])\n",
    "    # compute probabilities from feature vector and parameters\n",
    "    probs = softmax(forward(x, W, b))\n",
    "    # compute the gradients for biases and weights and take optimization step\n",
    "    dldb = probs.T - onehot(sentence[-1], V).flatten()\n",
    "    b_step = np.multiply(dldb,eta)\n",
    "    b -= b_step\n",
    "    # lookup active weight rows to update\n",
    "    for j in range(N):\n",
    "        if np.sum(x[j]) != 0:\n",
    "            W[:,j] -= b_step*x[j]\n",
    "    return W, b\n",
    "\n",
    "def train(X):\n",
    "    # initialize parameters\n",
    "    W = np.random.rand(V,N)\n",
    "    b = np.random.rand(V)\n",
    "    # for each word in test_data (except padding)\n",
    "    for sentence in tqdm(X):\n",
    "        for i in range(n+1, len(sentence)+1):\n",
    "            # compute step and update parameters\n",
    "            W, b = step(sentence[i-n-1:i], W, b)\n",
    "    # in case of unknown word, assume uniform distribution over 1e6 words\n",
    "    for i in range(UNK,N+UNK,V):\n",
    "        W[:,i] = np.array([1e-6]*V)\n",
    "    # return parameters\n",
    "    return (W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [07:00<00:00,  2.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train model   \n",
    "W, b = train(padded_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:40<00:00,  9.92it/s]\n",
      "100%|██████████| 1565/1565 [02:42<00:00,  9.60it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# score model on train and test sets\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# for each word in each sentence\n",
    "for sentence in tqdm(padded_train):\n",
    "    sentence_loss = 0\n",
    "    for i in range(n+1,len(sentence)):\n",
    "            # compute probability of word according to model\n",
    "            x = feat_func(sentence[i-n-1:i-1])\n",
    "            sentence_loss += LL_loss(softmax(forward(x, W, b))[sentence[i]])\n",
    "    # normalize by length of sentence\n",
    "    train_losses.append(sentence_loss/len(sentence))\n",
    "\n",
    "# as above\n",
    "for sentence in tqdm(padded_test):\n",
    "    sentence_loss = 0\n",
    "    for i in range(n+1,len(sentence)):\n",
    "            x = feat_func(sentence[i-n-1:i-1])\n",
    "            sentence_loss += LL_loss(softmax(forward(x, W, b))[sentence[i]])\n",
    "    test_losses.append(sentence_loss/len(sentence))\n",
    "\n",
    "# plot losses as histograms\n",
    "_ = plt.hist(train_losses,100,density=True,alpha=0.75,label=\"Train\")\n",
    "_ = plt.hist(test_losses,100,density=True,alpha=0.75,label=\"Test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prediction functions\n",
    "\n",
    "def greedy_search_step(past, W, b):\n",
    "    # compute score vector and choose highest score\n",
    "    x = feat_func(past[-n:])\n",
    "    scores = forward(x, W, b)\n",
    "    # prevent choosing padding token\n",
    "    scores[0] = 0\n",
    "    return np.argmax(scores)\n",
    "\n",
    "def random_search_step(past, W, b):\n",
    "    # compute score vector and sample from softmaxed distribution\n",
    "    x = feat_func(past[-n:])\n",
    "    scores = forward(x, W, b)\n",
    "    # prevent choosing padding token\n",
    "    scores[0] = 0\n",
    "    probs = softmax(scores)\n",
    "    return np.random.choice(range(V), p = probs)\n",
    "\n",
    "# predict sentence given seed and length\n",
    "def search(W, b, seed, length, strat=\"greedy\"):\n",
    "    sentence = [w2i[word] for word in seed.split()]\n",
    "    for _ in range(length):\n",
    "        # halt if end sentence token is reached\n",
    "        if sentence[-1] == END:\n",
    "            return \" \".join([i2w[i] for i in sentence])\n",
    "        # predict next word\n",
    "        if strat == \"greedy\":\n",
    "            prediction = greedy_search_step(sentence, W, b)\n",
    "        if strat == \"random\":\n",
    "            prediction = random_search_step(sentence, W, b)\n",
    "        # append to sentence\n",
    "        sentence.append(prediction)\n",
    "    return \" \".join([i2w[i] for i in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the little man had . </s>\n",
      "the little man had snap forces problems exchanged it so . </s>\n"
     ]
    }
   ],
   "source": [
    "# test generation\n",
    "\n",
    "seed = \"the little man had\"\n",
    "length = 10\n",
    "print(search(W, b, seed, length, \"greedy\"))\n",
    "print(search(W, b, seed, length, \"random\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the little woman had , and the in the , and the in the\n",
      "the little woman had of the are pool are london shredded the it about\n"
     ]
    }
   ],
   "source": [
    "seed = \"the little woman had\"\n",
    "length = 10\n",
    "print(search(W, b, seed, length, \"greedy\"))\n",
    "print(search(W, b, seed, length, \"random\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fin."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9026a92a5d7f7d8aa1c4ab30d4b010c040491f6b59d39fcb84365f121721c1f0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('coca1227': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
