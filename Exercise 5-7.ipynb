{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Language Model with PyTorch\n",
    "Exercise 5-6 from Neural Machine Translation and Sequence-to-sequence Models: A Tutorial by \n",
    "Graham Neubig (https://arxiv.org/abs/1703.01619)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# hidden layer width\n",
    "H = 100\n",
    "# embedding dimension\n",
    "E = 50\n",
    "# length of past-window\n",
    "n = 3\n",
    "# size of embedding vector, depends on feature function defined below\n",
    "N = n*E\n",
    "\n",
    "# load data\n",
    "X_train = open(\"en-de/train.en-de.low.filt.en\", \"r\", encoding=\"UTF-8\").readlines()\n",
    "X_test = open(\"en-de/test.en-de.low.en\", \"r\", encoding=\"UTF-8\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode vocab as numerical index\n",
    "padded_train = []\n",
    "# automatically register new words to new consequitive ids\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "# define special characters\n",
    "S = w2i[\"<s>\"]\n",
    "END = w2i[\"</s>\"]\n",
    "UNK = w2i[\"<UNK>\"]\n",
    "\n",
    "# pad, split, and encode training data (taking subset to avoid dimensionality explosion)\n",
    "for sentence in X_train[:1000]:\n",
    "    padded_train.append([S]*n)\n",
    "    for word in sentence.strip().split():\n",
    "        padded_train[-1].append(w2i[word])\n",
    "    padded_train[-1].append(END)\n",
    "\n",
    "VOCAB = list(w2i.keys())\n",
    "i2w = {v: k for k,v in w2i.items()}\n",
    "\n",
    "# vocab length\n",
    "V = len(VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_test = []\n",
    "\n",
    "# encode uknown words as UNK token\n",
    "w2i.default_factory = lambda: UNK\n",
    "\n",
    "# pad, split, and encode test data\n",
    "for sentence in X_test:\n",
    "        padded_test.append([S]*n + [w2i[word] for word in sentence.strip().split()] + [END])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFLM, self).__init__()\n",
    "        \n",
    "        # define the embedding layer\n",
    "        self.embedding = nn.Embedding(V,E)\n",
    "        # define the two fully-connected layers\n",
    "        self.fc1 = nn.Linear(N,H)\n",
    "        self.fc2 = nn.Linear(H,V)\n",
    "\n",
    "    def forward(self, indices):\n",
    "\n",
    "        # define forward pass, return scores\n",
    "        x = self.embedding(indices)\n",
    "        x = torch.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train(X):\n",
    "    # initialize network, loss, optimizer   \n",
    "    net = FFLM()\n",
    "    # Cross Entropy loss combines softmax, log, and NLL\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=1e-4)\n",
    "    running_loss = 0.0\n",
    "    # for each word in the training data (except padding)\n",
    "    for j, sentence in enumerate(tqdm(X)):\n",
    "        for i in range(n+1, len(sentence)+1):\n",
    "            input = torch.LongTensor(sentence[i-n-1:i-1])\n",
    "            label = torch.LongTensor([sentence[i-1]])\n",
    "            # zero parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            scores = net(input).view(1,-1)\n",
    "            # calculate loss\n",
    "            loss = criterion(scores, label)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # take step\n",
    "            optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if j % 100 == 99:\n",
    "            print(f\"{j+1}, loss: {running_loss / 100}\")\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # return trained model\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 100/1000 [00:30<04:46,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100, loss: 5.720907120704651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 201/1000 [00:58<03:49,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200, loss: 1.55138625562191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 300/1000 [01:31<03:50,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300, loss: 0.7542289109528064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 400/1000 [02:01<03:27,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400, loss: 0.6277357091754675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 501/1000 [02:25<03:03,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500, loss: 1.065086120814085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 601/1000 [02:58<01:36,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600, loss: 1.1030938270688058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 700/1000 [03:35<01:50,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700, loss: 1.0827897578477859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 800/1000 [04:20<01:50,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800, loss: 1.0870680314302446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 900/1000 [04:57<00:27,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900, loss: 1.1107256837189197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:28<00:00,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000, loss: 0.4400615897774696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = train(padded_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:23<00:00, 42.33it/s]\n",
      "100%|██████████| 1565/1565 [00:39<00:00, 39.64it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWRElEQVR4nO3df5Bd5XnY8e/D6lcNGLWS3BAtRHIqsEWHgL0VtTwOyIRaGDnCUzwVtmOIm1HlKTjY4wKxRx1a/YPxjH9gk2g0RFU8SSNSsINsKyalNRU0pGjlEsLyw1aARIvArJRhATcYLTz9Y+/Kl9Xdvefunrt379nvZ2ZH95zz3nOfM5IevXruc94TmYkkqfud1OkAJEnlMKFLUkWY0CWpIkzoklQRJnRJqoh5nfrgpUuX5ooVKzr18ZLUlQ4cOHAkM5c1OtaxhL5ixQr6+/s79fGS1JUi4m8nOlao5BIR6yPiyYg4GBE3Njj+HyLi4drPoxHxekT8k+kELUlqTdOEHhE9wG3ApcBq4MqIWF0/JjO/lJnnZeZ5wO8A/ysz/74N8UqSJlBkhr4GOJiZT2Xma8BuYOMk468E/riM4CRJxRWpoS8HDtVtDwIXNBoYEW8B1gPXTHB8M7AZ4Mwzz2wpUEk6duwYg4ODvPrqq50Ope0WLVpEb28v8+fPL/yeIgk9GuybaAGYDwH/e6JyS2buAHYA9PX1uYiMpJYMDg5y6qmnsmLFCiIapaZqyEyOHj3K4OAgK1euLPy+IiWXQeCMuu1e4PAEYzdhuUVSm7z66qssWbKk0skcICJYsmRJy/8TKZLQ9wOrImJlRCxgNGnvaRDAacCFwN0tRSBJLah6Mh8zletsWnLJzJGIuAa4B+gBdmbmQERsqR3fXhv6YeDPM/OnLUchSZq2QjcWZeZeYO+4fdvHbe8CdpUVmCQ1s2nHg6Web/fm90x6/OjRo1x88cUAPP/88/T09LBs2ehNmw899BALFiyY8L39/f1885vf5NZbby0v4HE6dqeo1G3qk0ezv/iqpiVLlvDwww8DcNNNN3HKKafwuc997vjxkZER5s1rnFb7+vro6+tra3wuziVJ03D11Vfz2c9+lnXr1nHDDTfw0EMPsXbtWs4//3zWrl3Lk08+CcB9993Hhg0bgNF/DD75yU9y0UUX8fa3v720WbszdEmaph/96Efce++99PT08NJLL7Fv3z7mzZvHvffey+c//3nuuuuuE97zxBNP8IMf/ICXX36Zs88+m0996lMt9Zw3YkKXpGn6yEc+Qk9PDwDDw8NcddVV/PjHPyYiOHbsWMP3XHbZZSxcuJCFCxfytre9jZ/85Cf09vZOKw5LLpI0TSeffPLx11u3bmXdunU8+uijfOc735mwl3zhwoXHX/f09DAyMjLtOEzoklSi4eFhli9fDsCuXbtm9LMtuUjqWrOx2+j666/nqquu4stf/jLvf//7Z/SzI7MzS6r09fWlD7hQN7FtsfMef/xx3vnOd3Y6jBnT6Hoj4kBmNux/tOQiSRVhQpekijChS1JFmNAlqSLscpG6nF/WaowzdEmqCGfokrrXrg3lnu/q7056eDrL58LoAl0LFixg7dq15cQ7jgldkgpqtnxuM/fddx+nnHJK2xK6JRdJmoYDBw5w4YUX8u53v5sPfOADPPfccwDceuutrF69mnPPPZdNmzbxzDPPsH37dr7yla9w3nnncf/995ceizN0SZqizOTaa6/l7rvvZtmyZdxxxx184QtfYOfOndx88808/fTTLFy4kBdffJHFixezZcuWlmf1rTChS9IU/exnP+PRRx/lkksuAeD111/n9NNPB+Dcc8/lYx/7GJdffjmXX375jMRjQpekKcpMzjnnHB588MRnm37ve99j37597Nmzh23btjEwMND2eKyhS9IULVy4kKGhoeMJ/dixYwwMDPDGG29w6NAh1q1bxy233MKLL77IK6+8wqmnnsrLL7/ctngKzdAjYj3wNaAHuD0zb24w5iLgq8B84EhmXlhalJLUSJM2w3Y76aSTuPPOO/n0pz/N8PAwIyMjXHfddZx11ll8/OMfZ3h4mMzkM5/5DIsXL+ZDH/oQV1xxBXfffTdf//rXed/73ldqPE0TekT0ALcBlwCDwP6I2JOZj9WNWQz8LrA+M/8uIt5WapSSNMvcdNNNx1/v27fvhOMPPPDACfvOOussHnnkkbbFVGSGvgY4mJlPAUTEbmAj8FjdmI8C38rMvwPIzBfKDlSqGm/ZV9mK1NCXA4fqtgdr++qdBfzjiLgvIg5ExCcanSgiNkdEf0T0Dw0NTS1iSVJDRRJ6NNg3/jFH84B3A5cBHwC2RsRZJ7wpc0dm9mVm39jtspLUik49ZW2mTeU6iyT0QeCMuu1e4HCDMd/PzJ9m5hFgH/ArLUcjSZNYtGgRR48erXxSz0yOHj3KokWLWnpfkRr6fmBVRKwEngU2MVozr3c38I2ImAcsAC4AvtJSJJLURG9vL4ODg8yFku2iRYvo7e1t6T1NE3pmjkTENcA9jLYt7szMgYjYUju+PTMfj4jvA48AbzDa2vhoy1cgSZOYP38+K1eu7HQYs1ahPvTM3AvsHbdv+7jtLwFfKi80SfXsilEz3ikqSRVhQpekijChS1JFuNqi1EW2Hrm+9qr8hyOo+zlDl6SKMKFLUkWY0CWpIkzoklQRJnRJqggTuiRVhAldkirCPnSpRGWst1LkHPVjpmXXhtFfO/xsTpXDGbokVYQJXZIqwoQuSRVhQpekijChS1JF2OWiyhrrBGm126SdTwaaqDultK4VzWnO0CWpIkzoklQRJnRJqggTuiRVRKEvRSNiPfA1oAe4PTNvHnf8IuBu4Onarm9l5n8uL0xpbjn+qLldp5VzW/7YLf6qtKYJPSJ6gNuAS4BBYH9E7MnMx8YNvT8z/VOjGVF2J4pdJqqCIiWXNcDBzHwqM18DdgMb2xuWJKlVRRL6cuBQ3fZgbd9474mIv4qIP4uIcxqdKCI2R0R/RPQPDQ1NIVxJ0kSK1NCjwb4ct/1D4Jcy85WI+CDwp8CqE96UuQPYAdDX1zf+HJIKOl5jB7YtvaWDkWg2KTJDHwTOqNvuBQ7XD8jMlzLzldrrvcD8iFhaWpSSpKaKJPT9wKqIWBkRC4BNwJ76ARHxCxERtddrauc9WnawkqSJNS25ZOZIRFwD3MNo2+LOzByIiC2149uBK4BPRcQI8A/Apsy0pCJN08DhYbbZgaOCCvWh18ooe8ft2173+hvAN8oNTWqj+r5sH7+mivBOUUmqCBO6JFWECV2SKsIHXEh1xvq7Z6q3235ylcmErsqbzrovRdZ4cR0YzRaWXCSpIkzoklQRllykLldfh4f7f/7SNdDnHGfoklQRJnRJqggTuiRVhAldkirChC5JFWFCl6SKsG1RmuXe3JY4A1xauGs5Q5ekinCGLrXJdNaQkabCGbokVYQzdEkTLxNgPb2rOEOXpIowoUtSRRRK6BGxPiKejIiDEXHjJOP+RUS8HhFXlBeiJKmIpjX0iOgBbgMuAQaB/RGxJzMfazDui8A97QhUKsNY58nWI8Oc84unTe9ktfry1iPDPj5Os0KRGfoa4GBmPpWZrwG7gY0Nxl0L3AW8UGJ8kqSCiiT05cChuu3B2r7jImI58GFg+2QniojNEdEfEf1DQ0OtxipJmkSRhB4N9uW47a8CN2Tm65OdKDN3ZGZfZvYtW7asYIiSpCKK9KEPAmfUbfcCh8eN6QN2RwTAUuCDETGSmX9aRpBSGWZ8TZT6z9x1Wm27zfV2Hzs3pxVJ6PuBVRGxEngW2AR8tH5AZq4cex0Ru4Dvmsw12w0cHgZgW90t+t1u7JqA6X/pq67TNKFn5khEXMNo90oPsDMzByJiS+34pHVzSdLMKHTrf2buBfaO29cwkWfm1dMPS5LUKtdykUrWrFbfiVq+5gZv/ZekijChS1JFWHLRrFD1h0HUd59I7WJCl5qor3nP6TVbXBt91rPkIkkVYUKXpIowoUtSRVhDlxqYqFd8/Nosc5b19FnJhK4ZVZVuFrtWNBtZcpGkinCGru6zawNbj9RWSiyhjbCMW/G9nV+zgTN0SaoIE7okVYQJXZIqwhq6Zq+CrXFV6ZyphLHfM1sZO8IZuiRVhAldkirChC5JFWENXZol7GXXdDlDl6SKKDRDj4j1wNeAHuD2zLx53PGNwDbgDWAEuC4zHyg5VnWRIp0n9WMa7R+7GxRgW93Y+v0qpn7tmXN+cY4vLFZhTRN6RPQAtwGXAIPA/ojYk5mP1Q37H8CezMyIOBf4E+Ad7QhYktRYkRn6GuBgZj4FEBG7gY3A8YSema/UjT8ZyDKDlIrYeuT60WVt63qgrUtrLilSQ18OHKrbHqzte5OI+HBEPAF8D/hkOeFJkooqktCjwb4TZuCZ+e3MfAdwOaP19BNPFLE5Ivojon9oaKilQCVJkyuS0AeBM+q2e4HDEw3OzH3AL0fE0gbHdmRmX2b2LVu2rOVgJUkTK1JD3w+sioiVwLPAJuCj9QMi4p8Bf1P7UvRdwALgaNnBqjtN1M3SNrX1ROyG0VzTNKFn5khEXAPcw2jb4s7MHIiILbXj24F/DXwiIo4B/wD8m8z0i1FJmkGF+tAzcy+wd9y+7XWvvwh8sdzQJEmt8E5RSaoIE7okVYQJXZIqwoQuSRVhQpekinA9dEnlm+h5sAWfE6upcYYuSRVhQpekijChS1JFWEOXKqr+KUWtjPWJRt3LGbokVYQJXZIqwpKLupqPmJN+zhm6JFWECV2SKsKELkkVYQ1ds4o18S5Ufzu/OsoZuiRVhAldkirChC5JFWFCl6SK8EtRaY5qZa0XdYdCM/SIWB8RT0bEwYi4scHxj0XEI7Wfv4iIXyk/VEnSZJom9IjoAW4DLgVWA1dGxOpxw54GLszMc4FtwI6yA5UkTa5IyWUNcDAznwKIiN3ARuCxsQGZ+Rd14/8S6C0zSMn+9C5mn/qMKVJyWQ4cqtserO2byL8F/qzRgYjYHBH9EdE/NDRUPEpJUlNFEno02JcNB0asYzSh39DoeGbuyMy+zOxbtmxZ8SglSU0VKbkMAmfUbfcCh8cPiohzgduBSzPzaDnhSSqbTzKqriIJfT+wKiJWAs8Cm4CP1g+IiDOBbwG/kZk/Kj1KdZ36mve2pbdMOmai45Ja0zShZ+ZIRFwD3AP0ADszcyAittSObwf+I7AE+N2IABjJzL72hS1JGq/QjUWZuRfYO27f9rrXvwX8VrmhSZJa4a3/klQR3vqv8hToN27UT26PuVQOE7oA2LTjweOvd29+z5THdJrrk8wMu19mJ0suklQRJnRJqggTuiRVhAldkirCL0UrYOzLynZ8mVn/3mZjth4Z9guyOcgvSGcPE7qmZ9cGth6xs0RTMFGb69Xfndk4KsSSiyRVhAldkirChC5JFWENXS3beuR62OWXX9JsY0LXlEx0i7233nc/fw+7lyUXSaoIE7okVYQll6qp7+2dJf28Lo8rzQxn6JJUESZ0SaoISy5dpMi6KmMGDg+zrYXxUjN2v8x+JvS5ZoIa+5vr3PfPXDySSlOo5BIR6yPiyYg4GBE3Njj+joh4MCJ+FhGfKz9MSVIzTWfoEdED3AZcAgwC+yNiT2Y+Vjfs74FPA5e3I0hJUnNFZuhrgIOZ+VRmvgbsBjbWD8jMFzJzP3CsDTFKkgooUkNfDhyq2x4ELpjKh0XEZmAzwJlnnjmVU8wuY/XoDvd7H69/u76KNKcVSejRYF9O5cMycwewA6Cvr29K55hrWuls6ebPVDX49KLOKlJyGQTOqNvuBQ63JxxJ0lQVSej7gVURsTIiFgCbgD3tDUuS1KqmJZfMHImIa4B7gB5gZ2YORMSW2vHtEfELQD/wVuCNiLgOWJ2ZL7UvdE3bBM90rO9J37b0lob7pRkxS76n6haFbizKzL3A3nH7tte9fp7RUowkqUNcy0WSKsJb/2dAfdfI7s3vaWl8O7k2h2aK3S8zw4SuSVk314xr9N3OLFznfzay5CJJFWFCl6SKsORSYQ3LJdYvVSWWYt7EGbokVYQz9CnatONBth4ZnvAb+yKdKq12vzRj14pmk1b/PI6Ntwtm6pyhS1JFOEPvhFrdb+uR4TfdWj9molvvJxojSeAMXZIqw4QuSRVhQpekirCGPkvUd7xs7WAcUrtNuxtr14bGa8NMsBz0XOpPd4YuSRVhQpekijChS1JFWENvozL6ye03l6Zpotp6I11eb3eGLkkV0ZUz9LLXQJns/PUafdbA4WG2tfCEobE1YDrF9V7UTdr9pKOpnr/dOWiqnKFLUkV05Qy9VA3qaxPOoHed1nSMNW+pixVZX33Xhjf//d81bmY/yfuajpmmQjP0iFgfEU9GxMGIuLHB8YiIW2vHH4mId5UfqiRpMk0TekT0ALcBlwKrgSsjYvW4YZcCq2o/m4HfKzlOSVITRWboa4CDmflUZr4G7AY2jhuzEfhmjvpLYHFEnF5yrJKkSRSpoS8HDtVtDwIXFBizHHiuflBEbGZ0Bg/wSkQ82VK0Ddzx76b19qXAkenGAGuPv7pjWmPaoqRrnNW8xu43+67vN6Ps9/38Gqd67lG/NNGBIgm90SfnFMaQmTuAHQU+c0ZERH9m9nU6jnbyGquh6tdY9euDmbnGIiWXQeCMuu1e4PAUxkiS2qhIQt8PrIqIlRGxANgE7Bk3Zg/wiVq3y78EhjPzufEnkiS1T9OSS2aORMQ1wD1AD7AzMwciYkvt+HZgL/BB4CDw/4DfbF/IpZo15Z828hqroerXWPXrgxm4xsg8odQtSepC3vovSRVhQpekipiTCT0izoiIH0TE4xExEBG/3emYyhYRiyLioYj4q9o1/qdOx9QOEdETEf83Irp7IesJRMQzEfHXEfFwRPR3Op52iIjFEXFnRDxR+zs5e5YvLEFEnF37/Rv7eSkirmvLZ83FGnrtLtbTM/OHEXEqcAC4PDMf63BopYmIAE7OzFciYj7wAPDbtTt5KyMiPgv0AW/NzBaeZNAdIuIZoC8zZ9dNNyWKiD8A7s/M22uddG/JzBc7HFZb1JZSeRa4IDP/tuzzz8kZemY+l5k/rL1+GXic0TtbK6O2DMMrtc35tZ9K/esdEb3AZcDtnY5FUxMRbwV+Ffh9gMx8rarJvOZi4G/akcxhjib0ehGxAjgf+D8dDqV0tXLEw8ALwH/PzKpd41eB64E3OhxHOyXw5xFxoLZ0RtW8HRgC/kutdHZ7RJzc6aDaaBPwx+06+ZxO6BFxCnAXcF1mvtTpeMqWma9n5nmM3rm7JiL+eYdDKk1EbABeyMwDnY6lzd6bme9idEXTfx8Rv9rpgEo2D3gX8HuZeT7wU+CEJbqroFZO+nXgv7XrM+ZsQq/Vle8C/igzv9XpeNqp9l/Y+4D1nY2kVO8Ffr1WY94NvD8i/rCzIZUvMw/Xfn0B+Dajq59WySAwWPe/xzsZTfBVdCnww8z8Sbs+YE4m9NoXhr8PPJ6ZX+50PO0QEcsiYnHt9T8Cfg14oqNBlSgzfyczezNzBaP/jf2fmfnxDodVqog4ufalPbUyxL8CHu1sVOXKzOeBQxFxdm3XxUBlmhPGuZI2lltg7j6C7r3AbwB/XasxA3w+M/d2LqTSnQ78Qe1b9ZOAP8nMSrb2Vdg/Bb49Ov9gHvBfM/P7nQ2pLa4F/qhWkniK7lk6pLCIeAtwCTC9Bb+bfc5cbFuUpCqakyUXSaoiE7okVYQJXZIqwoQuSRVhQpekijChS1JFmNAlqSL+P4kp6WWw9W53AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# score model on train and test sets\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# for each word in each sentence\n",
    "for sentence in tqdm(padded_train):\n",
    "    sentence_loss = 0\n",
    "    # compute probability of word according to model\n",
    "    for i in range(n+1,len(sentence)):\n",
    "            input = torch.LongTensor(sentence[i-n-1:i-1])\n",
    "            label = torch.LongTensor([sentence[i-1]])\n",
    "            # calculate scores\n",
    "            scores = net(input).view(1,V)\n",
    "            # compute the loss\n",
    "            sentence_loss += criterion(scores,label).item()\n",
    "    # normalize by length of sentence\n",
    "    train_losses.append(sentence_loss/len(sentence))\n",
    "\n",
    "# as above\n",
    "for sentence in tqdm(padded_test):\n",
    "    sentence_loss = 0\n",
    "    for i in range(n+1,len(sentence)):\n",
    "            input = torch.LongTensor(sentence[i-n-1:i-1])\n",
    "            label = torch.LongTensor([sentence[i-1]])\n",
    "            scores = net(input).view(1,V)\n",
    "            sentence_loss += criterion(scores,label).item()\n",
    "    test_losses.append(sentence_loss/len(sentence))\n",
    "\n",
    "# plot losses as histograms\n",
    "_ = plt.hist(train_losses,100,density=True,alpha=0.75,label=\"Train\")\n",
    "_ = plt.hist(test_losses,100,density=True,alpha=0.75,label=\"Test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prediction functions\n",
    "\n",
    "def greedy_search_step(net, past):\n",
    "    # compute score vector and choose highest score\n",
    "    input = torch.LongTensor(past)\n",
    "    scores = net(input)\n",
    "    # prevent choosing padding token\n",
    "    scores[0] = 0\n",
    "    return np.argmax(scores.detach().numpy())\n",
    "\n",
    "def random_search_step(net, past):\n",
    "    # compute score vector and sample from softmaxed distribution\n",
    "    input = torch.LongTensor(past)\n",
    "    scores = net(input)\n",
    "    # prevent choosing padding token\n",
    "    scores[0] = 0\n",
    "    probs = torch.softmax(scores,0)\n",
    "    return np.random.choice(range(V), p = probs.detach().numpy())\n",
    "\n",
    "# predict sentence given seed and length\n",
    "def search(net, seed, length, strat=\"greedy\"):\n",
    "    sentence = [w2i[word] for word in seed.split()]\n",
    "    for _ in range(length):\n",
    "        # halt if end sentence token is reached\n",
    "        if sentence[-1] == END:\n",
    "            return \" \".join([i2w[i] for i in sentence])\n",
    "        # predict next word\n",
    "        if strat == \"greedy\":\n",
    "            prediction = greedy_search_step(net, sentence[-n:])\n",
    "        if strat == \"random\":\n",
    "            prediction = random_search_step(net, sentence[-n:])\n",
    "        # append to sentence\n",
    "        sentence.append(prediction)\n",
    "    return \" \".join([i2w[i] for i in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the little man had and the 's of the , and the 's of\n",
      "the little man had either in '' -- try into a three it was\n"
     ]
    }
   ],
   "source": [
    "# test generation\n",
    "\n",
    "seed = \"the little man had\"\n",
    "length = 10\n",
    "print(search(net, seed, length, \"greedy\"))\n",
    "print(search(net, seed, length, \"random\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the little woman had , and the 's of the , and the 's\n",
      "the little woman had photographs not aspects up people has people , new look\n"
     ]
    }
   ],
   "source": [
    "seed = \"the little woman had\"\n",
    "length = 10\n",
    "print(search(net, seed, length, \"greedy\"))\n",
    "print(search(net, seed, length, \"random\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fin."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9026a92a5d7f7d8aa1c4ab30d4b010c040491f6b59d39fcb84365f121721c1f0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('coca1227': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
